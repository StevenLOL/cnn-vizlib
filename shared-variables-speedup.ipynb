{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This notebook is about using shared variables.\n",
    "# The upside of shared variables is that they can remain on the GPU between calls,\n",
    "# effectively removing the GPU transfer bottleneck when you have enough memory.\n",
    "#\n",
    "# The downside is that many libraries provide precompiled function expressions,\n",
    "# where can need to substitute real variables with shared variables somehow.\n",
    "#\n",
    "# Theano does not provide an elegant mechanism for doing so.\n",
    "#\n",
    "# There appears to be a way around this, assuming we *know* what input variable was used.\n",
    "# \n",
    "# Unfortunately I was not able to find a way to retrieve the input variables of a function.\n",
    "#\n",
    "# FunctionMaker.inputs does exist, but seems to be a long way from what theano.function actually returns.\n",
    "# However, a more elegant solution than the one presented here may exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Quadro K1000M (CNMeM is disabled, CuDNN not available)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import theano \n",
    "import theano.tensor as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.ivector('x')\n",
    "expr = x.sum()\n",
    "f = theano.function([x], [expr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = np.arange(100, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(4950)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b, e = T.iscalar('b'), T.iscalar('e')\n",
    "v_shared = theano.shared(v)\n",
    "f_batched = theano.function([b,e],f.outputs,givens={x: v_shared[b:e]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([array(4950)], [array(45)], [array(945)])\n"
     ]
    }
   ],
   "source": [
    "print(f_batched(0, 100), f_batched(0,10), f_batched(90,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the same trick on nolearn.lasagne's Neural network.\n",
    "#\n",
    "# The main work occurs in the train_loop function, and looks like this:\n",
    "\n",
    " for Xb, yb in self.batch_iterator_train(X_train, y_train):\n",
    "     batch_train_loss, accuracy = self.apply_batch_func(\n",
    "         self.train_iter_, Xb, yb)\n",
    "     train_losses.append(batch_train_loss)\n",
    "     train_accuracies.append(accuracy)\n",
    "\n",
    "     for func in on_batch_finished:\n",
    "         func(self, self.train_history_)\n",
    "\n",
    "# apply_batch_func seems to do nothing, other than simply calling the function:\n",
    "    @staticmethod\n",
    "    def apply_batch_func(func, Xb, yb=None):\n",
    "        if isinstance(Xb, dict):\n",
    "            kwargs = dict(Xb)\n",
    "            if yb is not None:\n",
    "                kwargs['y'] = yb\n",
    "            return func(**kwargs)\n",
    "        else:\n",
    "            return func(Xb) if yb is None else func(Xb, yb)\n",
    "\n",
    "# The self.train_iter function is initialized here:\n",
    "\n",
    "        iter_funcs = self._create_iter_funcs(\n",
    "            self.layers_, self.objective, self.update,\n",
    "            self.y_tensor_type,\n",
    "            )\n",
    "        self.train_iter_, self.eval_iter_, self.predict_iter_ = iter_funcs\n",
    "\n",
    "# And self._create_iter_funcs looks like this:\n",
    "\n",
    "    def _create_iter_funcs(self, layers, objective, update, output_type):\n",
    "        y_batch = output_type('y_batch')\n",
    "\n",
    "        output_layer = layers[-1]\n",
    "        objective_kw = self._get_params_for('objective')\n",
    "\n",
    "        loss_train = objective(\n",
    "            layers, target=y_batch, **objective_kw)\n",
    "        loss_eval = objective(\n",
    "            layers, target=y_batch, deterministic=True, **objective_kw)\n",
    "        predict_proba = get_output(output_layer, None, deterministic=True)\n",
    "        if not self.regression:\n",
    "            predict = predict_proba.argmax(axis=1)\n",
    "            accuracy = T.mean(T.eq(predict, y_batch))\n",
    "        else:\n",
    "            accuracy = loss_eval\n",
    "\n",
    "        all_params = self.get_all_params(trainable=True)\n",
    "        update_params = self._get_params_for('update')\n",
    "        updates = update(loss_train, all_params, **update_params)\n",
    "\n",
    "        input_layers = [layer for layer in layers.values()\n",
    "                        if isinstance(layer, InputLayer)]\n",
    "\n",
    "        X_inputs = [theano.Param(input_layer.input_var, name=input_layer.name)\n",
    "                    for input_layer in input_layers]\n",
    "        inputs = X_inputs + [theano.Param(y_batch, name=\"y\")]\n",
    "\n",
    "        train_iter = theano.function(\n",
    "            inputs=inputs,\n",
    "            outputs=[loss_train, accuracy],\n",
    "            updates=updates,\n",
    "            allow_input_downcast=True,\n",
    "            )\n",
    "        eval_iter = theano.function(\n",
    "            inputs=inputs,\n",
    "            outputs=[loss_eval, accuracy],\n",
    "            allow_input_downcast=True,\n",
    "            )\n",
    "        predict_iter = theano.function(\n",
    "            inputs=X_inputs,\n",
    "            outputs=predict_proba,\n",
    "            allow_input_downcast=True,\n",
    "            )\n",
    "\n",
    "        return train_iter, eval_iter, predict_iter\n",
    "\n",
    "# The inputs are as follows:\n",
    "# [inputs of input layers, input for y]\n",
    "#\n",
    "# self.batch_iterator_train needs to take two parameters and return (Xb, yb)\n",
    "# The output types do not matter.\n",
    "#\n",
    "# The functions in turn will be called as f(Xb, yb), \n",
    "# so whatever we compile our GPU functions to needs to take two arguments.\n",
    "#\n",
    "# Presumably we can make Xb and yb our batch indexes. They will be identical but nevertheless must be duplicated.\n",
    "#\n",
    "# The batch_iterator is set during construction:\n",
    "\n",
    "        batch_iterator_train=BatchIterator(batch_size=128),\n",
    "        batch_iterator_test=BatchIterator(batch_size=128),\n",
    "\n",
    "# Here BatchIterator is a class that will return batches based on slices.\n",
    "# We are better off writing our own BatchIterator.\n",
    "#\n",
    "# The BatchIterator is called on X_train,X_valid,y_train,y_valid,\n",
    "# which is a split produced by self.train_split(X,y,self)\n",
    "#\n",
    "# self.train_split is set in the constructor:\n",
    "        train_split=TrainSplit(eval_size=0.2),\n",
    "#\n",
    "# TrainSplit is a simple class that splits our train and test set.\n",
    "#\n",
    "# It would appear we are better off by passing in indices instead of data.\n",
    "# Then splitting and iteration will occur on theses indices.\n",
    "# These indices are then passed to our custom iterators, which used shared variables and indices as inputs.\n",
    "#\n",
    "# So all we have to do is overwrite:\n",
    "self.train_iter_, self.eval_iter_, self.predict_iter_\n",
    "# which can only be done after .initialize() is called,\n",
    "# which is normally done when .fit is called,\n",
    "# but we can call it manually beforehand since it sets a flag whether it has been called already,\n",
    "# and subsequent calls in .fit will not overwrite our behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nolearn.lasagne\n",
    "import lasagne\n",
    "\n",
    "class GpuNeuralNet(nolearn.lasagne.NeuralNet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        layers,\n",
    "        update=lasagne.updates.nesterov_momentum,\n",
    "        loss=None,  # BBB\n",
    "        objective=nolearn.lasagne.objective,\n",
    "        objective_loss_function=None,\n",
    "        batch_iterator_train=nolearn.lasagne.BatchIterator(batch_size=128),\n",
    "        batch_iterator_test=nolearn.lasagne.BatchIterator(batch_size=128),\n",
    "        regression=False,\n",
    "        max_epochs=100,\n",
    "        train_split=nolearn.lasagne.TrainSplit(eval_size=0.2),\n",
    "        custom_score=None,\n",
    "        X_tensor_type=None,\n",
    "        y_tensor_type=None,\n",
    "        use_label_encoder=False,\n",
    "        on_batch_finished=None,\n",
    "        on_epoch_finished=None,\n",
    "        on_training_started=None,\n",
    "        on_training_finished=None,\n",
    "        more_params=None,\n",
    "        verbose=0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(GpuNeuralNet, self).__init__(\n",
    "            layers,\n",
    "            update,\n",
    "            loss,\n",
    "            objective,\n",
    "            objective_loss_function,\n",
    "            batch_iterator_train,\n",
    "            batch_iterator_test,\n",
    "            regression,\n",
    "            max_epochs,\n",
    "            train_split,\n",
    "            custom_score,\n",
    "            X_tensor_type,\n",
    "            y_tensor_type,\n",
    "            use_label_encoder,\n",
    "            on_batch_finished,\n",
    "            on_epoch_finished,\n",
    "            on_training_started,\n",
    "            on_training_finished,\n",
    "            more_params,\n",
    "            verbose,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.X_shared = theano.shared(X)\n",
    "        self.y_shared = theano.shared(y)\n",
    "        \n",
    "    def _create_iter_funcs(self, layers, objective, update, output_type):\n",
    "        y_batch = output_type('y_batch')\n",
    "\n",
    "        output_layer = layers[-1]\n",
    "        objective_kw = self._get_params_for('objective')\n",
    "\n",
    "        loss_train = objective(\n",
    "            layers, target=y_batch, **objective_kw)\n",
    "        loss_eval = objective(\n",
    "            layers, target=y_batch, deterministic=True, **objective_kw)\n",
    "        predict_proba = get_output(output_layer, None, deterministic=True)\n",
    "        if not self.regression:\n",
    "            predict = predict_proba.argmax(axis=1)\n",
    "            accuracy = T.mean(T.eq(predict, y_batch))\n",
    "        else:\n",
    "            accuracy = loss_eval\n",
    "\n",
    "        all_params = self.get_all_params(trainable=True)\n",
    "        update_params = self._get_params_for('update')\n",
    "        updates = update(loss_train, all_params, **update_params)\n",
    "\n",
    "        input_layers = [layer for layer in layers.values()\n",
    "                        if isinstance(layer, InputLayer)]\n",
    "        assert len(input_layers) == 1, 'Multiple input layers not supported'\n",
    "        \n",
    "        # FIXME: Original code wraps all variables in params.\n",
    "        # For whatever reason Theano does not want this.\n",
    "        # Not sure what the effects are of not wrapping Variable in Param.\n",
    "        Xvar = input_layers[0].input_var\n",
    "        yvar = y_batch\n",
    "        \n",
    "        # FIXME: using vectors here is the easiest solution since the batch iterator\n",
    "        # does not need to change.\n",
    "        # However, it might not be faster than the original code.\n",
    "        # Should this not work then the BatchIterator needs to be adapted such that Xb, yb\n",
    "        # returns by its __iter__ func are scalars indicating the start of the batch.\n",
    "        #batch_idxs = [T.iscalar('Xidx'), T.iscalar('yidx')]\n",
    "        batch_idxs = [T.ivector('Xidx'), T.ivector('yidx')]\n",
    "        ix, iy = batch_idxs\n",
    "        bs = self.batch_iterator_train.batch_size\n",
    "\n",
    "        train_iter = theano.function(\n",
    "            inputs=batch_idxs,\n",
    "            outputs=[loss_train, accuracy],\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                #Xvar: self.X_shared[ix * bs: (ix + 1) * bs],\n",
    "                #yvar: self.y_shared[iy * bs: (iy + 1) * bs],\n",
    "                Xvar: self.X_shared[ix],\n",
    "                yvar: self.y_shared[iy],\n",
    "            },\n",
    "            allow_input_downcast=True,\n",
    "            )\n",
    "        \n",
    "        bs = self.batch_iterator_test.batch_size\n",
    "        eval_iter = theano.function(\n",
    "            inputs=batch_idxs,\n",
    "            outputs=[loss_eval, accuracy],\n",
    "            givens={\n",
    "                #Xvar: self.X_shared[ix * bs: (ix + 1) * bs],\n",
    "                #yvar: self.y_shared[iy * bs: (iy + 1) * bs],\n",
    "                Xvar: self.X_shared[ix],\n",
    "                yvar: self.y_shared[iy],\n",
    "            },\n",
    "            allow_input_downcast=True,\n",
    "            )\n",
    "        \n",
    "        # Uses the same input as eval_iter\n",
    "        predict_iter = theano.function(\n",
    "            inputs=batch_idxs[0:-1],\n",
    "            outputs=predict_proba,\n",
    "            givens={\n",
    "                #Xvar: self.X_shared[ix * bs: (ix + 1) * bs],\n",
    "                Xvar: self.X_shared[ix]\n",
    "            },\n",
    "            allow_input_downcast=True,\n",
    "            )\n",
    "\n",
    "        return train_iter, eval_iter, predict_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import vizlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1, 32, 32), 6)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = vizlib.data.counting_2d()\n",
    "ds.X.shape, len(set(ds.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = InputLayer((None, 1, 32, 32))\n",
    "conv_layer = Conv2DLayer(input_layer, num_filters=1, filter_size=(3,3))\n",
    "dense_layer = DenseLayer(conv_layer, num_units=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = GpuNeuralNet(ds.X, ds.y, layers=dense_layer, update_learning_rate=1e-2, update_momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.7457330227\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "Xidx = np.arange(len(ds), dtype=theano.config.floatX)\n",
    "yidx = Xidx.astype(np.int32)\n",
    "nn.fit(Xidx, yidx)\n",
    "t1 = time.time()\n",
    "print(t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn2 = nolearn.lasagne.NeuralNet(layers=dense_layer, update_learning_rate=1e-3, update_momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.071755171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/nolearn/lasagne/base.py:428: UserWarning: The Param class is deprecated. Replace Param(default=N) by theano.In(value=N)\n",
      "  for input_layer in input_layers]\n",
      "/usr/local/lib/python2.7/dist-packages/nolearn/lasagne/base.py:429: UserWarning: The Param class is deprecated. Replace Param(default=N) by theano.In(value=N)\n",
      "  inputs = X_inputs + [theano.Param(y_batch, name=\"y\")]\n"
     ]
    }
   ],
   "source": [
    "t00 = time.time()\n",
    "nn2.fit(ds.X, ds.y)\n",
    "t01 = time.time()\n",
    "print(t01 - t00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So it seems the speedup is alright, but rather marginal.\n",
    "# I wonder if we can improve by switching to integer input.\n",
    "import nolearn.lasagne\n",
    "import lasagne\n",
    "\n",
    "# This batch iterator is rather silly\n",
    "# It has to match the interface used by nolearn.lasagne,\n",
    "# but really it is rather simply looping over the start indices of the batches.\n",
    "class DummyBatchIterator(object):\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __call__(self, X, y=None):\n",
    "        self.X, self.y = X, y\n",
    "        return self\n",
    "\n",
    "    def __iter__(self):\n",
    "        bs = self.batch_size\n",
    "        for i in range((self.n_samples + bs - 1) // bs):\n",
    "            yield i, i\n",
    "\n",
    "    @property\n",
    "    def n_samples(self):\n",
    "        X = self.X\n",
    "        if isinstance(X, dict):\n",
    "            return len(list(X.values())[0])\n",
    "        else:\n",
    "            return len(X)\n",
    "\n",
    "class GpuNeuralNet2(nolearn.lasagne.NeuralNet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        layers,\n",
    "        update=lasagne.updates.nesterov_momentum,\n",
    "        loss=None,  # BBB\n",
    "        objective=nolearn.lasagne.objective,\n",
    "        objective_loss_function=None,\n",
    "        batch_iterator_train=DummyBatchIterator(batch_size=128),\n",
    "        batch_iterator_test=DummyBatchIterator(batch_size=128),\n",
    "        regression=False,\n",
    "        max_epochs=100,\n",
    "        train_split=nolearn.lasagne.TrainSplit(eval_size=0.2),\n",
    "        custom_score=None,\n",
    "        X_tensor_type=None,\n",
    "        y_tensor_type=None,\n",
    "        use_label_encoder=False,\n",
    "        on_batch_finished=None,\n",
    "        on_epoch_finished=None,\n",
    "        on_training_started=None,\n",
    "        on_training_finished=None,\n",
    "        more_params=None,\n",
    "        verbose=0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(GpuNeuralNet2, self).__init__(\n",
    "            layers,\n",
    "            update,\n",
    "            loss,\n",
    "            objective,\n",
    "            objective_loss_function,\n",
    "            batch_iterator_train,\n",
    "            batch_iterator_test,\n",
    "            regression,\n",
    "            max_epochs,\n",
    "            train_split,\n",
    "            custom_score,\n",
    "            X_tensor_type,\n",
    "            y_tensor_type,\n",
    "            use_label_encoder,\n",
    "            on_batch_finished,\n",
    "            on_epoch_finished,\n",
    "            on_training_started,\n",
    "            on_training_finished,\n",
    "            more_params,\n",
    "            verbose,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.X_shared = theano.shared(X)\n",
    "        self.y_shared = theano.shared(y)\n",
    "        \n",
    "    def _create_iter_funcs(self, layers, objective, update, output_type):\n",
    "        y_batch = output_type('y_batch')\n",
    "\n",
    "        output_layer = layers[-1]\n",
    "        objective_kw = self._get_params_for('objective')\n",
    "\n",
    "        loss_train = objective(\n",
    "            layers, target=y_batch, **objective_kw)\n",
    "        loss_eval = objective(\n",
    "            layers, target=y_batch, deterministic=True, **objective_kw)\n",
    "        predict_proba = get_output(output_layer, None, deterministic=True)\n",
    "        if not self.regression:\n",
    "            predict = predict_proba.argmax(axis=1)\n",
    "            accuracy = T.mean(T.eq(predict, y_batch))\n",
    "        else:\n",
    "            accuracy = loss_eval\n",
    "\n",
    "        all_params = self.get_all_params(trainable=True)\n",
    "        update_params = self._get_params_for('update')\n",
    "        updates = update(loss_train, all_params, **update_params)\n",
    "\n",
    "        input_layers = [layer for layer in layers.values()\n",
    "                        if isinstance(layer, InputLayer)]\n",
    "        assert len(input_layers) == 1, 'Multiple input layers not supported'\n",
    "        \n",
    "        # FIXME: Original code wraps all variables in params.\n",
    "        # For whatever reason Theano does not want this.\n",
    "        # Not sure what the effects are of not wrapping Variable in Param.\n",
    "        Xvar = input_layers[0].input_var\n",
    "        yvar = y_batch\n",
    "        \n",
    "        batch_idxs = [T.iscalar('Xidx'), T.iscalar('yidx')]\n",
    "        ix, iy = batch_idxs\n",
    "        bs = self.batch_iterator_train.batch_size\n",
    "\n",
    "        train_iter = theano.function(\n",
    "            inputs=batch_idxs,\n",
    "            outputs=[loss_train, accuracy],\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                Xvar: self.X_shared[ix * bs: (ix + 1) * bs],\n",
    "                yvar: self.y_shared[iy * bs: (iy + 1) * bs],\n",
    "            },\n",
    "            allow_input_downcast=True,\n",
    "            )\n",
    "        \n",
    "        bs = self.batch_iterator_test.batch_size\n",
    "        eval_iter = theano.function(\n",
    "            inputs=batch_idxs,\n",
    "            outputs=[loss_eval, accuracy],\n",
    "            givens={\n",
    "                Xvar: self.X_shared[ix * bs: (ix + 1) * bs],\n",
    "                yvar: self.y_shared[iy * bs: (iy + 1) * bs],\n",
    "            },\n",
    "            allow_input_downcast=True,\n",
    "            )\n",
    "        \n",
    "        # Uses the same input as eval_iter\n",
    "        predict_iter = theano.function(\n",
    "            inputs=batch_idxs[0:-1],\n",
    "            outputs=predict_proba,\n",
    "            givens={\n",
    "                Xvar: self.X_shared[ix * bs: (ix + 1) * bs],\n",
    "            },\n",
    "            allow_input_downcast=True,\n",
    "            )\n",
    "\n",
    "        return train_iter, eval_iter, predict_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.897287846\n"
     ]
    }
   ],
   "source": [
    "nn2 = GpuNeuralNet2(ds.X, ds.y, layers=dense_layer, update_learning_rate=1e-3, update_momentum=0.9)\n",
    "t10 = time.time()\n",
    "nn2.fit(ds.X, ds.y)\n",
    "t11 = time.time()\n",
    "print(t11 - t10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seems even slower\n",
    "# Try to get a definate answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 46.5 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "input_layer = InputLayer((None, 1, 32, 32))\n",
    "conv_layer = Conv2DLayer(input_layer, num_filters=1, filter_size=(3,3))\n",
    "dense_layer = DenseLayer(conv_layer, num_units=6)\n",
    "nn = GpuNeuralNet(ds.X, ds.y, layers=dense_layer, update_learning_rate=1e-2, update_momentum=0.9)\n",
    "X = np.arange(len(ds.X), dtype=theano.config.floatX)\n",
    "y = np.arange(len(ds.y), dtype=np.int32)\n",
    "nn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 44.1 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "input_layer = InputLayer((None, 1, 32, 32))\n",
    "conv_layer = Conv2DLayer(input_layer, num_filters=1, filter_size=(3,3))\n",
    "dense_layer = DenseLayer(conv_layer, num_units=6)\n",
    "nn2 = GpuNeuralNet2(ds.X, ds.y, layers=dense_layer, update_learning_rate=1e-2, update_momentum=0.9)\n",
    "nn2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 1min 51s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "input_layer = InputLayer((None, 1, 32, 32))\n",
    "conv_layer = Conv2DLayer(input_layer, num_filters=1, filter_size=(3,3))\n",
    "dense_layer = DenseLayer(conv_layer, num_units=6)\n",
    "nn3 = nolearn.lasagne.NeuralNet(layers=dense_layer, update_learning_rate=1e-2, update_momentum=0.9)\n",
    "nn3.fit(ds.X, ds.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Local machine (Quadro K1000M) (single filter):\n",
    "# GpuNeuralNet  |  46.5\n",
    "# GpuNeuralNet2 |  44.1\n",
    "# NeuralNet     | 111.0\n",
    "#\n",
    "#\n",
    "# Romulus (K40c) (20 filters):\n",
    "# GpuNeuralNet    | 104s\n",
    "# GpuNeuralNet2   | 333s\n",
    "# NeuralNet       | 340s\n",
    "#\n",
    "#\n",
    "# So GpuNeuralNet is the fastest implementation (which is using VECTORS instead of SLICES -- rather strange).\n",
    "# Nevertheless, this is the one I implemented in vizlib."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
